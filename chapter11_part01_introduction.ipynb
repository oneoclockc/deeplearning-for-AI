{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oneoclockc/deeplearning-for-AI/blob/main/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxQ4h4kugk76"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT3rigD0gk77"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- text classification : 다음 text의 주제는?\n",
        "- content filtering : 이 텍스트가 abuse를 포함하는가?\n",
        "- sentiment analsis : 이 text가 positive / negative?\n",
        "- laguage modeling : 다음 문장의 다음에 올 단어는?\n",
        "- translation : 번역\n",
        "- summarization : 요약"
      ],
      "metadata": {
        "id": "LQbvD0azwvw-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCt5Vcs4gk77"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YckyRrKdgk78"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  sunset came. I was satring at the Mexico sky. Isnt nature splendid?\n",
        "- 소문자 변환, 문장부호 제거  \n",
        "> sunset came i was staring at the mexico sky isnt nature splendid\n",
        "- 용어의 변형(시제 등)을 단일 표현으로 변환  \n",
        "> sunset came i was stare at the mexico sky isnt nature splendid"
      ],
      "metadata": {
        "id": "HORdYHy1xz58"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EyALPOrgk78"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- word-level tokenization : 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환\n",
        "- N-gram tokenization : N-gram은 n개의 연속 단어 그룹\n",
        "- character-level tokenization : 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환(거의 사용 x)"
      ],
      "metadata": {
        "id": "VEChw22OzEB2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rga2foeNgk79"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 토큰을 numeric vector 로 변경. one-hot encoding과 token imbedding이 대표적\n",
        "- one-hot encoding : 모든 단어에 고유한 정수 인덱스를 부여, 이 정수 인덱스 i를 크기가 N(어휘사전의 크기)인 이진 벡터로 변환 후 i번째 원소는 1로, 나머지는 0으로 인코딩"
      ],
      "metadata": {
        "id": "9xarCU1c0_Y9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBA4W6Nqgk79"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "asUHfYGKgk7-"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower() # 소문자 변환 \n",
        "        return \"\".join(char for char in text if char not in string.punctuation) # 구두점 제거 \n",
        "\n",
        "    def tokenize(self, text): \n",
        "        text = self.standardize(text) # standardize 후 공백 기준으로 split\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1} # 1: out of vocabulary(i cant recognize it) / 0 : mask token(ignore it. im not a word!)\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary: # vocabulary에 없는 단어에 대해 등록\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rc0H_CBJgk8A",
        "outputId": "c3b2af1d-2aea-4e9e-9ae5-76b361bf8d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence) # still -> cannot recognize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dCSvySCYgk8B",
        "outputId": "4ecfff65-1a25-48dd-ad88-436325a8a0fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MbK745nrgk8B"
      },
      "outputs": [],
      "source": [
        "# 실제로는 keras의 textvectorization layer를 사용\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\", # 정수 단위로 단어를 인코딩 \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RJQL_qizgk8C"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor) # 소문자 변환 \n",
        "    return tf.strings.regex_replace( # 구두점 문자를 빈 문자열로 변환(제거)\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor): \n",
        "    return tf.strings.split(string_tensor) # 공백 기준으로 문장 분할\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GGfZ8y6Vgk8C"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset) # text corpus의 어휘를 인덱스화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D7dskaJgk8D"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Md4iJsdXgk8D",
        "outputId": "eaaafe6c-cd1e-4130-a9d2-658d0434d235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YGijAX01gk8D",
        "outputId": "568b4aef-45c5-44b8-877f-229dc421ed58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence) # 인코딩된 문장 확인 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b7VsGMArgk8D",
        "outputId": "ea415ce9-f569-4f2b-b847-1397369b3e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence) # 문장으로 다시 디코딩 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9QQoyvgk8E"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bag-of-word model : 단어의 순서 고려 x  \n",
        "RNN, transformers : sequence models"
      ],
      "metadata": {
        "id": "hzuB4y2s6NqP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWVGO2Y5gk8E"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g-3WbK7egk8E",
        "outputId": "ac16ed25-799f-417e-cc3c-9f06523e9cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  15.1M      0  0:00:05  0:00:05 --:--:-- 19.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QCW-stjbgk8E"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NXNlfLkHgk8F",
        "outputId": "c0b23bc0-3e1e-4b3f-e4e2-735b33491e57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ej-bi5kwgk8F"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files) # 시드 설정 \n",
        "    num_val_samples = int(0.2 * len(files)) # val set -> train set에서 20% 할당\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ASgkckh5gk8F",
        "outputId": "afcebb85-7cb4-456c-bf2f-a845e44418bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ") # dataset 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjemrHQvgk8F"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gx2eeZsogk8G",
        "outputId": "228dc2e6-154e-4093-afda-7d8972e245bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Worry not, Disney fans--this special edition DVD of the beloved Cinderella won\\'t turn into a pumpkin at the strike of midnight. One of the most enduring animated films of all time, the Disney-fide adaptation of the gory Brothers Grimm fairy tale became a classic in its own right, thanks to some memorable tunes (including \"A Dream Is a Wish Your Heart Makes,\" \"Bibbidi-Bobbidi-Boo,\" and the title song) and some endearingly cute comic relief. The famous slipper (click for larger image) We all know the story--the wicked stepmother and stepsisters simply won\\'t have it, this uppity Cinderella thinking she\\'s going to a ball designed to find the handsome prince an appropriate sweetheart, but perseverance, animal buddies, and a well-timed entrance by a fairy godmother make sure things turn out all right. There are a few striking sequences of pure animation--for example, Cinderella is reflected in bubbles drifting through the air--and the design is rich and evocative throughout. It\\'s a simple story padded here agreeably with comic business, particularly Cinderella\\'s rodent pals (dressed up conspicuously like the dwarf sidekicks of another famous Disney heroine) and their misadventures with a wretched cat named Lucifer. There\\'s also much harrumphing and exposition spouting by the King and the Grand Duke. It\\'s a much simpler and more graceful work than the more frenetically paced animated films of today, which makes it simultaneously quaint and highly gratifying.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHz95ijSgk8G"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi7CX31Lgk8G"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VjEWIV5gk8G"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hTGR-TpQgk8G"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000, # 어휘를 가장 빈도수가 높은 20000개로 제한\n",
        "    output_mode=\"multi_hot\", # multi hot binary vector로 인코딩 \n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) # raw text input만 생성하는 데이터 집합을 준비\n",
        "text_vectorization.adapt(text_only_train_ds) # 데이터 집합 인덱싱\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), # processed dataset\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Ov-LOpgk8H"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2eg8-VbLgk8H",
        "outputId": "e91d2d11-7ffe-494f-bcbc-b40ffd9b521a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suzfevljgk8H"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FtwL3gh1gk8H"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGz8EJLagk8H"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "814D7LSngk8H",
        "outputId": "1f6167ae-40b7-44ad-d3bf-08d1051f9b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 19ms/step - loss: 0.4150 - accuracy: 0.8273 - val_loss: 0.2956 - val_accuracy: 0.8840\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2837 - accuracy: 0.8947 - val_loss: 0.2925 - val_accuracy: 0.8880\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2485 - accuracy: 0.9126 - val_loss: 0.2992 - val_accuracy: 0.8916\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2470 - accuracy: 0.9189 - val_loss: 0.3150 - val_accuracy: 0.8814\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2356 - accuracy: 0.9219 - val_loss: 0.3257 - val_accuracy: 0.8802\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2313 - accuracy: 0.9238 - val_loss: 0.3366 - val_accuracy: 0.8784\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2205 - accuracy: 0.9282 - val_loss: 0.3388 - val_accuracy: 0.8810\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2250 - accuracy: 0.9277 - val_loss: 0.3482 - val_accuracy: 0.8818\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2195 - accuracy: 0.9323 - val_loss: 0.3527 - val_accuracy: 0.8766\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2176 - accuracy: 0.9313 - val_loss: 0.3593 - val_accuracy: 0.8784\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2951 - accuracy: 0.8826\n",
            "Test acc: 0.883\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(), # cache()를 통해 메모리에 저장, 데이터가 메모리에 들어갈 정도로 작은 경우에만 수행 가능 \n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV-stBIzgk8I"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvg4-4-ggk8I"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dgjmNupNgk8I"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6BG57_fgk8I"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4cMRGF1ugk8I",
        "outputId": "071d80c9-bf0d-49d7-a875-9ec20b1648a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.3868 - accuracy: 0.8396 - val_loss: 0.2704 - val_accuracy: 0.8966\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2406 - accuracy: 0.9125 - val_loss: 0.2744 - val_accuracy: 0.8936\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2095 - accuracy: 0.9315 - val_loss: 0.2864 - val_accuracy: 0.8976\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1899 - accuracy: 0.9404 - val_loss: 0.2990 - val_accuracy: 0.9004\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1864 - accuracy: 0.9433 - val_loss: 0.3139 - val_accuracy: 0.8990\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1792 - accuracy: 0.9482 - val_loss: 0.3262 - val_accuracy: 0.9004\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1826 - accuracy: 0.9507 - val_loss: 0.3306 - val_accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1766 - accuracy: 0.9514 - val_loss: 0.3448 - val_accuracy: 0.8976\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1740 - accuracy: 0.9521 - val_loss: 0.3492 - val_accuracy: 0.8946\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1742 - accuracy: 0.9538 - val_loss: 0.3534 - val_accuracy: 0.8956\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2690 - accuracy: 0.9004\n",
            "Test acc: 0.900\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW5OtMV1gk8J"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyhhQ36ggk8J"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "708KcgR9gk8J"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOgBFcO3gk8J"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PzwVQCVKgk8J"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fzV2cT5gk8J"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9YvDIFx4gk8J",
        "outputId": "2c4b891b-5dad-4b32-da62-8008e94faecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.4630 - accuracy: 0.7961 - val_loss: 0.3232 - val_accuracy: 0.8664\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3026 - accuracy: 0.8767 - val_loss: 0.2888 - val_accuracy: 0.8898\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2691 - accuracy: 0.8909 - val_loss: 0.3249 - val_accuracy: 0.8726\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2480 - accuracy: 0.8979 - val_loss: 0.3184 - val_accuracy: 0.8908\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2243 - accuracy: 0.9079 - val_loss: 0.3375 - val_accuracy: 0.8970\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2166 - accuracy: 0.9154 - val_loss: 0.3390 - val_accuracy: 0.8892\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2092 - accuracy: 0.9154 - val_loss: 0.3832 - val_accuracy: 0.8802\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2021 - accuracy: 0.9197 - val_loss: 0.3601 - val_accuracy: 0.8834\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2053 - accuracy: 0.9183 - val_loss: 0.3808 - val_accuracy: 0.8762\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1936 - accuracy: 0.9247 - val_loss: 0.3853 - val_accuracy: 0.8826\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.2787 - accuracy: 0.8941\n",
            "Test acc: 0.894\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oE_c_9rigk8K"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "udkdiWtNgk8K",
        "outputId": "57a8e6a7-ad9b-4cb8-ad6f-f4f64cea4849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.64 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part01_introduction.i",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}